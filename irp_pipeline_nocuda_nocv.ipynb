{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas_ta as ta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "root = tk.Tk()\n",
    "root.wm_attributes('-topmost', 1)\n",
    "root.withdraw()\n",
    "\n",
    "# feature engineering and scaling\n",
    "def prep(dataset):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    macro_path1 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\IRP DATASET  - Copy of US_macroeconomics.csv\"\n",
    "    macro_path2 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\^VIX.csv\"\n",
    "    macro_path3 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\CORESTICKM159SFRBATL.csv\"\n",
    "    macro_path4 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\FFR.csv\"\n",
    "    macro_path5 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\GDP.csv\"\n",
    "    macro_path6 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\VIX9D_History.csv\"\n",
    "    macro_path7 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\GVZ_History.csv\"\n",
    "    macro_path8 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\OVX_History.csv\"\n",
    "    macro_path9 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\VVIX_History.csv\"\n",
    "    macro_data1 = pd.read_csv(macro_path1)\n",
    "    macro_data2 = pd.read_csv(macro_path2)\n",
    "    macro_data3 = pd.read_csv(macro_path3)\n",
    "    macro_data4 = pd.read_csv(macro_path4)\n",
    "    macro_data5 = pd.read_csv(macro_path5)\n",
    "    macro_data6 = pd.read_csv(macro_path6)\n",
    "    macro_data7 = pd.read_csv(macro_path7)\n",
    "    macro_data8 = pd.read_csv(macro_path8)\n",
    "    macro_data9 = pd.read_csv(macro_path9)\n",
    "\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'], dayfirst=True)\n",
    "    dataset.set_index('Date', inplace=True)\n",
    "    dataset['Target'] = np.where(dataset['Close'].shift(-1) > dataset['Close'], 1, 0)\n",
    "\n",
    "    macro_data1['date'] = pd.to_datetime(macro_data1['date'], dayfirst=True)\n",
    "    macro_data1.set_index('date', inplace=True)\n",
    "\n",
    "    macro_data2['Date'] = pd.to_datetime(macro_data2['Date'], dayfirst=True)\n",
    "    macro_data2.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data3['Date'] = pd.to_datetime(macro_data3['Date'], dayfirst=True)\n",
    "    macro_data3.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data4['Date'] = pd.to_datetime(macro_data4['Date'], dayfirst=True)\n",
    "    macro_data4.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data5['Date'] = pd.to_datetime(macro_data5['Date'], dayfirst=True)\n",
    "    macro_data5.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data7['Date'] = pd.to_datetime(macro_data7['Date'], dayfirst=False)\n",
    "    macro_data7.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data8['Date'] = pd.to_datetime(macro_data8['Date'], dayfirst=False)\n",
    "    macro_data8.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data9['Date'] = pd.to_datetime(macro_data9['Date'])\n",
    "    macro_data9.set_index('Date', inplace=True)\n",
    "\n",
    "    merge_data1 = dataset.join(macro_data1, how='left').fillna(method='ffill')\n",
    "    merge_data2 = merge_data1.join(macro_data2, how='left').fillna(method='ffill')\n",
    "    merge_data3 = merge_data2.join(macro_data3, how='left').fillna(method='ffill')\n",
    "    merge_data4 = merge_data3.join(macro_data4, how='left').fillna(method='ffill')\n",
    "    merge_data5 = merge_data4.join(macro_data5, how='left').fillna(method='ffill')\n",
    "    merge_data7 = merge_data5.join(macro_data7, how='left').fillna(method='ffill')\n",
    "    merge_data8 = merge_data7.join(macro_data8, how='left').fillna(method='ffill')\n",
    "    merge_final = merge_data8.join(macro_data9, how='left').fillna(method='ffill')\n",
    "\n",
    "    merge_final['RSI (14D)'] = ta.rsi(merge_final['Close'], length=14)\n",
    "    merge_final['20 Day CCI'] = ta.cci(high=merge_final['High'], low=merge_final['Low'], \n",
    "                                    close=merge_final['Close'], length=20)\n",
    "    merge_final['Williams %R'] = ta.willr(high=merge_final['High'], low=merge_final['Low'], \n",
    "                                        close=merge_final['Close'], length=14)\n",
    "    merge_final['EMA (5D)'] = merge_final['Close'].ewm(span=5, adjust=False).mean()\n",
    "    merge_final['MA50'] = merge_final['Close'].rolling(window=50).mean()\n",
    "\n",
    "    final_dataset = merge_final.dropna()\n",
    "\n",
    "    time_shifted = final_dataset.resample('M').asfreq().dropna().head(102)\n",
    "    \n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'CPI', \n",
    "                'Mortgage_rate', 'Unemp_rate', 'disposable_income','GVZ', 'OVX', 'VVIX', \n",
    "                'RSI (14D)', '20 Day CCI', 'Williams %R', 'EMA (5D)', 'MA50', 'FFR']\n",
    "    \n",
    "    time_shifted[features] = time_shifted[features].astype(float)\n",
    "    time_shifted[features] = scaler.fit_transform(time_shifted[features])\n",
    "\n",
    "    return time_shifted, scaler\n",
    "\n",
    "# get dataset\n",
    "file_path = filedialog.askopenfilename(parent=root, title=\"Select A File\")\n",
    "ticker = pd.read_csv(file_path)\n",
    "ticker, scaler = prep(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "vif_ticker = pd.DataFrame()\n",
    "features = ticker[['Close', 'Volume', 'CPI', \n",
    "                'Mortgage_rate', 'Unemp_rate', 'disposable_income','GVZ', 'OVX', 'VVIX', \n",
    "                'RSI (14D)', '20 Day CCI', 'Williams %R', 'MA50', 'FFR']]\n",
    "vif_ticker[\"feature\"] = features.columns\n",
    "\n",
    "vif_ticker[\"VIF\"] = [variance_inflation_factor(features.values, i)\n",
    "                          for i in range(len(features.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1 loss: 0.6929\n",
      "Epoch 26 loss: 0.6818\n",
      "Epoch 51 loss: 0.4769\n",
      "Epoch 76 loss: 0.3940\n",
      "\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.5000\n",
      "Recall: 1.0000\n",
      "F1: 0.6667\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 2\n",
      "Epoch 1 loss: 0.6948\n",
      "Epoch 26 loss: 0.6957\n",
      "Epoch 51 loss: 0.6801\n",
      "Epoch 76 loss: 0.5372\n",
      "\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.5000\n",
      "Recall: 1.0000\n",
      "F1: 0.6667\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 3\n",
      "Epoch 1 loss: 0.6935\n",
      "Epoch 26 loss: 0.6902\n",
      "Epoch 51 loss: 0.6567\n",
      "Epoch 76 loss: 0.4883\n",
      "\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1: 0.0000\n",
      "Kappa: -0.2308\n",
      "\n",
      "Fold 4\n",
      "Epoch 1 loss: 0.7162\n",
      "Epoch 26 loss: 0.7078\n",
      "Epoch 51 loss: 0.6739\n",
      "Epoch 76 loss: 0.3663\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.7500\n",
      "Recall: 0.6000\n",
      "F1: 0.6667\n",
      "Kappa: 0.2500\n",
      "\n",
      "Fold 5\n",
      "Epoch 1 loss: 0.6973\n",
      "Epoch 26 loss: 0.6663\n",
      "Epoch 51 loss: 0.7578\n",
      "Epoch 76 loss: 0.5670\n",
      "\n",
      "Accuracy: 0.7500\n",
      "Precision: 0.6667\n",
      "Recall: 1.0000\n",
      "F1: 0.8000\n",
      "Kappa: 0.5000\n",
      "\n",
      "Fold 6\n",
      "Epoch 1 loss: 0.7047\n",
      "Epoch 26 loss: 0.5897\n",
      "Epoch 51 loss: 0.5502\n",
      "Epoch 76 loss: 0.4466\n",
      "\n",
      "Accuracy: 0.2500\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1: 0.0000\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 7\n",
      "Epoch 1 loss: 0.6954\n",
      "Epoch 26 loss: 0.6276\n",
      "Epoch 51 loss: 0.6108\n",
      "Epoch 76 loss: 0.4564\n",
      "\n",
      "Accuracy: 0.7500\n",
      "Precision: 1.0000\n",
      "Recall: 0.5000\n",
      "F1: 0.6667\n",
      "Kappa: 0.5000\n",
      "\n",
      "Fold 8\n",
      "Epoch 1 loss: 0.6979\n",
      "Epoch 26 loss: 0.6288\n",
      "Epoch 51 loss: 0.5139\n",
      "Epoch 76 loss: 0.9246\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.6250\n",
      "Recall: 1.0000\n",
      "F1: 0.7692\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 9\n",
      "Epoch 1 loss: 0.6982\n",
      "Epoch 26 loss: 0.6469\n",
      "Epoch 51 loss: 0.3969\n",
      "Epoch 76 loss: 0.4718\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.8000\n",
      "Recall: 0.6667\n",
      "F1: 0.7273\n",
      "Kappa: 0.1429\n",
      "\n",
      "Fold 10\n",
      "Epoch 1 loss: 0.6975\n",
      "Epoch 26 loss: 0.5591\n",
      "Epoch 51 loss: 0.5757\n",
      "Epoch 76 loss: 0.7303\n",
      "\n",
      "Accuracy: 0.7500\n",
      "Precision: 0.7500\n",
      "Recall: 1.0000\n",
      "F1: 0.8571\n",
      "Kappa: 0.0000\n",
      "\n",
      "\n",
      "Cross-Validation Results:\n",
      "Average Accuracy: 0.5875\n",
      "Average Precision: 0.5592\n",
      "Average Recall: 0.6767\n",
      "Average F1: 0.5820\n",
      "Average Kappa: 0.1162\n"
     ]
    }
   ],
   "source": [
    "# create LSTM model class\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer, num_layers=4, dropout=0.3):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers with dropout\n",
    "        self.lstm = nn.LSTM(input_layer, hidden_layer, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.linear_layer = nn.Linear(hidden_layer, output_layer)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, input_tensor.size(0), self.hidden_layer).to(input_tensor.device)\n",
    "        c0 = torch.zeros(self.num_layers, input_tensor.size(0), self.hidden_layer).to(input_tensor.device)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        out, _ = self.lstm(input_tensor, (h0, c0))\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        out = self.linear_layer(out)\n",
    "        return out\n",
    "\n",
    "# create sequences for input data and corresponding labels\n",
    "def create_sequence(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(0, len(input_data) - sequence_length):\n",
    "        sequence = input_data[i : i + sequence_length, :-1]\n",
    "        label = input_data[i + sequence_length, -1]\n",
    "        sequences.append((sequence, label))\n",
    "    return sequences\n",
    "\n",
    "# train the model with data provided\n",
    "def trainer(model, train_data, loss_func, opt, epochs, fold, losses):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for sequence, labels in train_data:\n",
    "            opt.zero_grad()\n",
    "            sequence = sequence.clone().detach().float()\n",
    "            labels = labels.clone().detach().float().view(-1, 1)\n",
    "            \n",
    "            y = model(sequence)\n",
    "            loss = loss_func(y, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 10 == 1 or epoch == 99:\n",
    "            print(f'Epoch {epoch} loss: {loss.item():.4f}')\n",
    "\n",
    "            new_row = pd.DataFrame({'Fold': [fold+1], 'Epoch': [epoch], 'Loss': [loss.item()]})\n",
    "            losses = pd.concat([losses, new_row], ignore_index=True)\n",
    "            \n",
    "    return losses\n",
    "\n",
    "def predictor(model, test_data):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for sequence, _ in test_data:\n",
    "            sequence = sequence.clone().detach().float()\n",
    "            y = model(sequence)\n",
    "            batch_predictions = torch.sigmoid(y)\n",
    "\n",
    "            # Convert predictions to tensor if they are scalar or float\n",
    "            if isinstance(batch_predictions, float) or batch_predictions.ndimension() == 0:\n",
    "                batch_predictions = torch.tensor([batch_predictions])\n",
    "\n",
    "            batch_predictions = torch.round(batch_predictions)\n",
    "\n",
    "            predictions.extend(batch_predictions.squeeze().tolist())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# create sequences\n",
    "sequence_length = 10\n",
    "sequences = create_sequence(ticker[['Close', 'Volume', 'CPI', 'Mortgage_rate', 'Unemp_rate', \n",
    "                                    'disposable_income','GVZ', 'OVX', 'VVIX', 'RSI (14D)', \n",
    "                                    '20 Day CCI', 'Williams %R', 'MA50', 'FFR', 'Target']].values, sequence_length)\n",
    "\n",
    "# cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "kappas = []\n",
    "losses = pd.DataFrame(columns=['Fold', 'Epoch', 'Loss'])\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(sequences)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    train_sequences = [sequences[i] for i in train_index]\n",
    "    test_sequences = [sequences[i] for i in test_index]\n",
    "    \n",
    "    train_data = torch.utils.data.DataLoader(train_sequences, shuffle=True, batch_size=32)\n",
    "    test_data = torch.utils.data.DataLoader(test_sequences, shuffle=False, batch_size=32)\n",
    "    \n",
    "    # initialize model\n",
    "    model = LSTM_Model(input_layer=14, hidden_layer=50, output_layer=1)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    \n",
    "    # train model\n",
    "    epochs = 100\n",
    "    losses = trainer(model, train_data, loss_func, opt, epochs, fold, losses)\n",
    "    \n",
    "    # make predictions\n",
    "    test_labels = [label for _, label in test_sequences]\n",
    "    predictions = predictor(model, test_data)\n",
    "    \n",
    "    # calculate statistics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    kappa = cohen_kappa_score(test_labels, predictions)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    kappas.append(kappa)\n",
    "    \n",
    "    print(f'\\nAccuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1: {f1:.4f}')\n",
    "    print(f'Kappa: {kappa:.4f}\\n')\n",
    "\n",
    "# average scores across all folds\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1s)\n",
    "avg_kappa = np.mean(kappas)\n",
    "\n",
    "print(f'\\nCross-Validation Results:')\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n",
    "print(f'Average Recall: {avg_recall:.4f}')\n",
    "print(f'Average F1: {avg_f1:.4f}')\n",
    "print(f'Average Kappa: {avg_kappa:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days rising: 61\n",
      "Number of days falling: 41\n",
      "Rise % is: 59.80%\n",
      "Fall % is: 40.20%\n"
     ]
    }
   ],
   "source": [
    "fall = (ticker.Target == 0).sum()\n",
    "rise = (ticker.Target == 1).sum()\n",
    "\n",
    "rise_per = (rise / (rise + fall)) * 100\n",
    "fall_per = (fall / (rise + fall)) * 100\n",
    "\n",
    "print(f'Number of days rising: {rise}')\n",
    "print(f'Number of days falling: {fall}')\n",
    "\n",
    "print(f'Rise % is: {rise_per:.2f}%')\n",
    "print(f'Fall % is: {fall_per:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F1</th>\n",
       "      <th>Average Kappa</th>\n",
       "      <th>Number of Rises</th>\n",
       "      <th>Number of Falls</th>\n",
       "      <th>Rise %</th>\n",
       "      <th>Fall %</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>^NYA</th>\n",
       "      <td>0.575</td>\n",
       "      <td>0.544167</td>\n",
       "      <td>0.651667</td>\n",
       "      <td>0.551526</td>\n",
       "      <td>0.06</td>\n",
       "      <td>61</td>\n",
       "      <td>41</td>\n",
       "      <td>59.803922</td>\n",
       "      <td>40.196078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.697520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.686410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0.553355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>0.301002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.701244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0.697063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>0.673104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>0.478574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>0.689138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>0.608989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>0.530336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.682015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.635720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>0.499969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>0.518369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.692602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>0.685756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>0.477065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>0.645523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>0.553089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>76</td>\n",
       "      <td>0.523920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.697268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>0.633234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>0.537804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>0.512548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.697266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0.639954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>0.726779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>0.513026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.707288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.702352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>0.512155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>76</td>\n",
       "      <td>0.447802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.681219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.599441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>0.405557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>76</td>\n",
       "      <td>0.481526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Average Accuracy  Average Precision  Average Recall  Average F1  \\\n",
       "Dataset                                                                    \n",
       "^NYA                0.575           0.544167        0.651667    0.551526   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "\n",
       "         Average Kappa Number of Rises Number of Falls     Rise %     Fall %  \\\n",
       "Dataset                                                                        \n",
       "^NYA              0.06              61              41  59.803922  40.196078   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "NaN                NaN             NaN             NaN        NaN        NaN   \n",
       "\n",
       "        Fold Epoch      Loss  \n",
       "Dataset                       \n",
       "^NYA     NaN   NaN       NaN  \n",
       "NaN        1     1  0.697520  \n",
       "NaN        1    26  0.686410  \n",
       "NaN        1    51  0.553355  \n",
       "NaN        1    76  0.301002  \n",
       "NaN        2     1  0.701244  \n",
       "NaN        2    26  0.697063  \n",
       "NaN        2    51  0.673104  \n",
       "NaN        2    76  0.478574  \n",
       "NaN        3     1  0.693932  \n",
       "NaN        3    26  0.689138  \n",
       "NaN        3    51  0.608989  \n",
       "NaN        3    76  0.530336  \n",
       "NaN        4     1  0.682015  \n",
       "NaN        4    26  0.635720  \n",
       "NaN        4    51  0.499969  \n",
       "NaN        4    76  0.518369  \n",
       "NaN        5     1  0.692602  \n",
       "NaN        5    26  0.685756  \n",
       "NaN        5    51  0.538335  \n",
       "NaN        5    76  0.477065  \n",
       "NaN        6     1  0.688972  \n",
       "NaN        6    26  0.645523  \n",
       "NaN        6    51  0.553089  \n",
       "NaN        6    76  0.523920  \n",
       "NaN        7     1  0.697268  \n",
       "NaN        7    26  0.633234  \n",
       "NaN        7    51  0.537804  \n",
       "NaN        7    76  0.512548  \n",
       "NaN        8     1  0.697266  \n",
       "NaN        8    26  0.639954  \n",
       "NaN        8    51  0.726779  \n",
       "NaN        8    76  0.513026  \n",
       "NaN        9     1  0.707288  \n",
       "NaN        9    26  0.702352  \n",
       "NaN        9    51  0.512155  \n",
       "NaN        9    76  0.447802  \n",
       "NaN       10     1  0.681219  \n",
       "NaN       10    26  0.599441  \n",
       "NaN       10    51  0.405557  \n",
       "NaN       10    76  0.481526  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Dataset', 'Average Accuracy', 'Average Precision', \n",
    "                                'Average Recall', 'Average F1', 'Average Kappa', 'Number of Rises', \n",
    "                                'Number of Falls', 'Rise %', 'Fall %'])\n",
    "\n",
    "datatset_name = (file_path.split('/')[-1].split('.')[0])\n",
    "\n",
    "metrics = {'Dataset': datatset_name ,'Average Accuracy': avg_accuracy, \n",
    "           'Average Precision': avg_precision, 'Average Recall': avg_recall, \n",
    "           'Average F1': avg_f1, 'Average Kappa': avg_kappa, 'Number of Rises': rise, 'Number of Falls': fall, \n",
    "           'Rise %': rise_per, 'Fall %': fall_per}\n",
    "\n",
    "export_results = results._append(metrics, ignore_index=True)\n",
    "\n",
    "export_results = pd.concat([export_results, losses], ignore_index=True)\n",
    "\n",
    "export_results.set_index('Dataset', inplace=True)\n",
    "\n",
    "export_results.to_csv(r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\IRP DATA\\\\\" + datatset_name + '_MONTHLY_LOSS.csv')\n",
    "\n",
    "export_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
