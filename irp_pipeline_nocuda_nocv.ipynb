{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:70: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_data1 = dataset.join(macro_data1, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:71: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_data2 = merge_data1.join(macro_data2, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:72: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_data3 = merge_data2.join(macro_data3, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:73: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_data4 = merge_data3.join(macro_data4, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:74: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_data5 = merge_data4.join(macro_data5, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:75: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_data7 = merge_data5.join(macro_data7, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:76: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_data8 = merge_data7.join(macro_data8, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:77: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merge_final = merge_data8.join(macro_data9, how='left').fillna(method='ffill')\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  time_shifted[features] = time_shifted[features].astype(float)\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\2806024467.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  time_shifted[features] = scaler.fit_transform(time_shifted[features])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Target</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Mortgage_rate</th>\n",
       "      <th>Unemp_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>FFR</th>\n",
       "      <th>GDP</th>\n",
       "      <th>GVZ</th>\n",
       "      <th>OVX</th>\n",
       "      <th>VVIX</th>\n",
       "      <th>RSI (14D)</th>\n",
       "      <th>20 Day CCI</th>\n",
       "      <th>Williams %R</th>\n",
       "      <th>EMA (5D)</th>\n",
       "      <th>MA50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-09-18</th>\n",
       "      <td>-1.3093</td>\n",
       "      <td>-1.3101</td>\n",
       "      <td>-1.3058</td>\n",
       "      <td>-1.3082</td>\n",
       "      <td>-1.3082</td>\n",
       "      <td>1.7228</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.4824</td>\n",
       "      <td>1.4564</td>\n",
       "      <td>1.6576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5857</td>\n",
       "      <td>14448.8820</td>\n",
       "      <td>1.1173</td>\n",
       "      <td>0.2701</td>\n",
       "      <td>-1.4931</td>\n",
       "      <td>1.4074</td>\n",
       "      <td>1.1448</td>\n",
       "      <td>0.8895</td>\n",
       "      <td>-1.3149</td>\n",
       "      <td>-1.3656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-21</th>\n",
       "      <td>-1.3089</td>\n",
       "      <td>-1.3137</td>\n",
       "      <td>-1.3116</td>\n",
       "      <td>-1.3113</td>\n",
       "      <td>-1.3113</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.4824</td>\n",
       "      <td>1.4564</td>\n",
       "      <td>1.6576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5919</td>\n",
       "      <td>14448.8820</td>\n",
       "      <td>1.2433</td>\n",
       "      <td>0.2543</td>\n",
       "      <td>-1.4555</td>\n",
       "      <td>1.1813</td>\n",
       "      <td>0.7634</td>\n",
       "      <td>0.7470</td>\n",
       "      <td>-1.3136</td>\n",
       "      <td>-1.3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-22</th>\n",
       "      <td>-1.3095</td>\n",
       "      <td>-1.3082</td>\n",
       "      <td>-1.3040</td>\n",
       "      <td>-1.3053</td>\n",
       "      <td>-1.3053</td>\n",
       "      <td>1.3468</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.4824</td>\n",
       "      <td>1.4564</td>\n",
       "      <td>1.6576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5919</td>\n",
       "      <td>14448.8820</td>\n",
       "      <td>1.5182</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>-1.6243</td>\n",
       "      <td>1.3705</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>1.0211</td>\n",
       "      <td>-1.3108</td>\n",
       "      <td>-1.3595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-23</th>\n",
       "      <td>-1.3041</td>\n",
       "      <td>-1.3028</td>\n",
       "      <td>-1.3091</td>\n",
       "      <td>-1.3145</td>\n",
       "      <td>-1.3145</td>\n",
       "      <td>1.6437</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.4824</td>\n",
       "      <td>1.4564</td>\n",
       "      <td>1.6576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5919</td>\n",
       "      <td>14448.8820</td>\n",
       "      <td>1.4028</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>-1.6293</td>\n",
       "      <td>0.7244</td>\n",
       "      <td>0.5992</td>\n",
       "      <td>0.4317</td>\n",
       "      <td>-1.3120</td>\n",
       "      <td>-1.3568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-24</th>\n",
       "      <td>-1.3127</td>\n",
       "      <td>-1.3145</td>\n",
       "      <td>-1.3215</td>\n",
       "      <td>-1.3231</td>\n",
       "      <td>-1.3231</td>\n",
       "      <td>1.6163</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.4824</td>\n",
       "      <td>1.4564</td>\n",
       "      <td>1.6576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5981</td>\n",
       "      <td>14448.8820</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>-1.4691</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>-0.0701</td>\n",
       "      <td>-1.3157</td>\n",
       "      <td>-1.3548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-23</th>\n",
       "      <td>2.5118</td>\n",
       "      <td>2.5037</td>\n",
       "      <td>2.5260</td>\n",
       "      <td>2.5028</td>\n",
       "      <td>2.5028</td>\n",
       "      <td>-0.4703</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7557</td>\n",
       "      <td>1.3415</td>\n",
       "      <td>-1.0050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6332</td>\n",
       "      <td>28629.1530</td>\n",
       "      <td>-0.3897</td>\n",
       "      <td>-0.6052</td>\n",
       "      <td>-0.3766</td>\n",
       "      <td>-0.0559</td>\n",
       "      <td>-0.1565</td>\n",
       "      <td>-0.9968</td>\n",
       "      <td>2.5126</td>\n",
       "      <td>2.4566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-24</th>\n",
       "      <td>2.4613</td>\n",
       "      <td>2.4384</td>\n",
       "      <td>2.4142</td>\n",
       "      <td>2.3936</td>\n",
       "      <td>2.3936</td>\n",
       "      <td>-0.0069</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7557</td>\n",
       "      <td>1.3415</td>\n",
       "      <td>-1.0050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6332</td>\n",
       "      <td>28629.1530</td>\n",
       "      <td>-0.2764</td>\n",
       "      <td>-0.5988</td>\n",
       "      <td>0.7084</td>\n",
       "      <td>-1.2518</td>\n",
       "      <td>-1.5002</td>\n",
       "      <td>-2.0064</td>\n",
       "      <td>2.4748</td>\n",
       "      <td>2.4602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-25</th>\n",
       "      <td>2.3958</td>\n",
       "      <td>2.4245</td>\n",
       "      <td>2.3894</td>\n",
       "      <td>2.3699</td>\n",
       "      <td>2.3699</td>\n",
       "      <td>0.6658</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7557</td>\n",
       "      <td>1.3415</td>\n",
       "      <td>-1.0050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6332</td>\n",
       "      <td>28629.1530</td>\n",
       "      <td>-0.3687</td>\n",
       "      <td>-0.6199</td>\n",
       "      <td>0.6012</td>\n",
       "      <td>-1.4515</td>\n",
       "      <td>-1.6974</td>\n",
       "      <td>-2.0030</td>\n",
       "      <td>2.4417</td>\n",
       "      <td>2.4632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26</th>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.4218</td>\n",
       "      <td>2.4234</td>\n",
       "      <td>2.4208</td>\n",
       "      <td>2.4208</td>\n",
       "      <td>-0.3261</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7557</td>\n",
       "      <td>1.3415</td>\n",
       "      <td>-1.0050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6332</td>\n",
       "      <td>28629.1530</td>\n",
       "      <td>-0.4401</td>\n",
       "      <td>-0.4276</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.8529</td>\n",
       "      <td>-1.2954</td>\n",
       "      <td>-1.3065</td>\n",
       "      <td>2.4366</td>\n",
       "      <td>2.4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>2.4364</td>\n",
       "      <td>2.4168</td>\n",
       "      <td>2.4527</td>\n",
       "      <td>2.4343</td>\n",
       "      <td>2.4343</td>\n",
       "      <td>-3.8648</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7557</td>\n",
       "      <td>1.3415</td>\n",
       "      <td>-1.0050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6332</td>\n",
       "      <td>28629.1530</td>\n",
       "      <td>-0.4401</td>\n",
       "      <td>-0.4276</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.7050</td>\n",
       "      <td>-1.1242</td>\n",
       "      <td>-1.1210</td>\n",
       "      <td>2.4377</td>\n",
       "      <td>2.4698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3739 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low   Close  Adj Close  Volume  Target     CPI  \\\n",
       "Date                                                                            \n",
       "2009-09-18 -1.3093 -1.3101 -1.3058 -1.3082    -1.3082  1.7228       0 -1.4824   \n",
       "2009-09-21 -1.3089 -1.3137 -1.3116 -1.3113    -1.3113  0.6899       1 -1.4824   \n",
       "2009-09-22 -1.3095 -1.3082 -1.3040 -1.3053    -1.3053  1.3468       0 -1.4824   \n",
       "2009-09-23 -1.3041 -1.3028 -1.3091 -1.3145    -1.3145  1.6437       0 -1.4824   \n",
       "2009-09-24 -1.3127 -1.3145 -1.3215 -1.3231    -1.3231  1.6163       0 -1.4824   \n",
       "...            ...     ...     ...     ...        ...     ...     ...     ...   \n",
       "2024-07-23  2.5118  2.5037  2.5260  2.5028     2.5028 -0.4703       0  1.7557   \n",
       "2024-07-24  2.4613  2.4384  2.4142  2.3936     2.3936 -0.0069       0  1.7557   \n",
       "2024-07-25  2.3958  2.4245  2.3894  2.3699     2.3699  0.6658       1  1.7557   \n",
       "2024-07-26  2.4000  2.4218  2.4234  2.4208     2.4208 -0.3261       1  1.7557   \n",
       "2024-07-29  2.4364  2.4168  2.4527  2.4343     2.4343 -3.8648       0  1.7557   \n",
       "\n",
       "            Mortgage_rate  Unemp_rate  ...     FFR        GDP     GVZ     OVX  \\\n",
       "Date                                   ...                                      \n",
       "2009-09-18         1.4564      1.6576  ... -0.5857 14448.8820  1.1173  0.2701   \n",
       "2009-09-21         1.4564      1.6576  ... -0.5919 14448.8820  1.2433  0.2543   \n",
       "2009-09-22         1.4564      1.6576  ... -0.5919 14448.8820  1.5182  0.1945   \n",
       "2009-09-23         1.4564      1.6576  ... -0.5919 14448.8820  1.4028  0.2443   \n",
       "2009-09-24         1.4564      1.6576  ... -0.5981 14448.8820  0.9872  0.4231   \n",
       "...                   ...         ...  ...     ...        ...     ...     ...   \n",
       "2024-07-23         1.3415     -1.0050  ...  2.6332 28629.1530 -0.3897 -0.6052   \n",
       "2024-07-24         1.3415     -1.0050  ...  2.6332 28629.1530 -0.2764 -0.5988   \n",
       "2024-07-25         1.3415     -1.0050  ...  2.6332 28629.1530 -0.3687 -0.6199   \n",
       "2024-07-26         1.3415     -1.0050  ...  2.6332 28629.1530 -0.4401 -0.4276   \n",
       "2024-07-29         1.3415     -1.0050  ...  2.6332 28629.1530 -0.4401 -0.4276   \n",
       "\n",
       "              VVIX  RSI (14D)  20 Day CCI  Williams %R  EMA (5D)    MA50  \n",
       "Date                                                                      \n",
       "2009-09-18 -1.4931     1.4074      1.1448       0.8895   -1.3149 -1.3656  \n",
       "2009-09-21 -1.4555     1.1813      0.7634       0.7470   -1.3136 -1.3624  \n",
       "2009-09-22 -1.6243     1.3705      0.8375       1.0211   -1.3108 -1.3595  \n",
       "2009-09-23 -1.6293     0.7244      0.5992       0.4317   -1.3120 -1.3568  \n",
       "2009-09-24 -1.4691     0.1950      0.1579      -0.0701   -1.3157 -1.3548  \n",
       "...            ...        ...         ...          ...       ...     ...  \n",
       "2024-07-23 -0.3766    -0.0559     -0.1565      -0.9968    2.5126  2.4566  \n",
       "2024-07-24  0.7084    -1.2518     -1.5002      -2.0064    2.4748  2.4602  \n",
       "2024-07-25  0.6012    -1.4515     -1.6974      -2.0030    2.4417  2.4632  \n",
       "2024-07-26 -0.0008    -0.8529     -1.2954      -1.3065    2.4366  2.4669  \n",
       "2024-07-29 -0.0008    -0.7050     -1.1242      -1.1210    2.4377  2.4698  \n",
       "\n",
       "[3739 rows x 29 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas_ta as ta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "root = tk.Tk()\n",
    "root.wm_attributes('-topmost', 1)\n",
    "root.withdraw()\n",
    "\n",
    "# feature engineering and scaling\n",
    "def prep(dataset):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    macro_path1 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\IRP DATASET  - Copy of US_macroeconomics.csv\"\n",
    "    macro_path2 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\^VIX.csv\"\n",
    "    macro_path3 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\CORESTICKM159SFRBATL.csv\"\n",
    "    macro_path4 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\FFR.csv\"\n",
    "    macro_path5 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\GDP.csv\"\n",
    "    macro_path6 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\VIX9D_History.csv\"\n",
    "    macro_path7 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\GVZ_History.csv\"\n",
    "    macro_path8 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\OVX_History.csv\"\n",
    "    macro_path9 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\VVIX_History.csv\"\n",
    "    macro_data1 = pd.read_csv(macro_path1)\n",
    "    macro_data2 = pd.read_csv(macro_path2)\n",
    "    macro_data3 = pd.read_csv(macro_path3)\n",
    "    macro_data4 = pd.read_csv(macro_path4)\n",
    "    macro_data5 = pd.read_csv(macro_path5)\n",
    "    macro_data6 = pd.read_csv(macro_path6)\n",
    "    macro_data7 = pd.read_csv(macro_path7)\n",
    "    macro_data8 = pd.read_csv(macro_path8)\n",
    "    macro_data9 = pd.read_csv(macro_path9)\n",
    "\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'], dayfirst=True)\n",
    "    dataset.set_index('Date', inplace=True)\n",
    "    dataset['Target'] = np.where(dataset['Close'].shift(-1) > dataset['Close'], 1, 0)\n",
    "\n",
    "    macro_data1['date'] = pd.to_datetime(macro_data1['date'], dayfirst=True)\n",
    "    macro_data1.set_index('date', inplace=True)\n",
    "\n",
    "    macro_data2['Date'] = pd.to_datetime(macro_data2['Date'], dayfirst=True)\n",
    "    macro_data2.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data3['Date'] = pd.to_datetime(macro_data3['Date'], dayfirst=True)\n",
    "    macro_data3.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data4['Date'] = pd.to_datetime(macro_data4['Date'], dayfirst=True)\n",
    "    macro_data4.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data5['Date'] = pd.to_datetime(macro_data5['Date'], dayfirst=True)\n",
    "    macro_data5.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data7['Date'] = pd.to_datetime(macro_data7['Date'], dayfirst=False)\n",
    "    macro_data7.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data8['Date'] = pd.to_datetime(macro_data8['Date'], dayfirst=False)\n",
    "    macro_data8.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data9['Date'] = pd.to_datetime(macro_data9['Date'])\n",
    "    macro_data9.set_index('Date', inplace=True)\n",
    "\n",
    "    merge_data1 = dataset.join(macro_data1, how='left').fillna(method='ffill')\n",
    "    merge_data2 = merge_data1.join(macro_data2, how='left').fillna(method='ffill')\n",
    "    merge_data3 = merge_data2.join(macro_data3, how='left').fillna(method='ffill')\n",
    "    merge_data4 = merge_data3.join(macro_data4, how='left').fillna(method='ffill')\n",
    "    merge_data5 = merge_data4.join(macro_data5, how='left').fillna(method='ffill')\n",
    "    merge_data7 = merge_data5.join(macro_data7, how='left').fillna(method='ffill')\n",
    "    merge_data8 = merge_data7.join(macro_data8, how='left').fillna(method='ffill')\n",
    "    merge_final = merge_data8.join(macro_data9, how='left').fillna(method='ffill')\n",
    "\n",
    "    merge_final['RSI (14D)'] = ta.rsi(merge_final['Close'], length=14)\n",
    "    merge_final['20 Day CCI'] = ta.cci(high=merge_final['High'], low=merge_final['Low'], \n",
    "                                    close=merge_final['Close'], length=20)\n",
    "    merge_final['Williams %R'] = ta.willr(high=merge_final['High'], low=merge_final['Low'], \n",
    "                                        close=merge_final['Close'], length=14)\n",
    "    merge_final['EMA (5D)'] = merge_final['Close'].ewm(span=5, adjust=False).mean()\n",
    "    merge_final['MA50'] = merge_final['Close'].rolling(window=50).mean()\n",
    "\n",
    "    final_dataset = merge_final.dropna()\n",
    "\n",
    "    time_shifted = final_dataset#.resample('M').asfreq().dropna()\n",
    "    \n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'CPI', \n",
    "                'Mortgage_rate', 'Unemp_rate', 'disposable_income','GVZ', 'OVX', 'VVIX', \n",
    "                'RSI (14D)', '20 Day CCI', 'Williams %R', 'EMA (5D)', 'MA50', 'FFR']\n",
    "    \n",
    "    time_shifted[features] = time_shifted[features].astype(float)\n",
    "    time_shifted[features] = scaler.fit_transform(time_shifted[features])\n",
    "\n",
    "    return time_shifted, scaler\n",
    "\n",
    "# get dataset\n",
    "file_path = filedialog.askopenfilename(parent=root, title=\"Select A File\")\n",
    "ticker = pd.read_csv(file_path)\n",
    "ticker, scaler = prep(ticker)\n",
    "\n",
    "ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_ticker = pd.DataFrame()\n",
    "features = ticker[['Close', 'Volume', 'CPI', \n",
    "                'Mortgage_rate', 'Unemp_rate', 'disposable_income','GVZ', 'OVX', 'VVIX', \n",
    "                'RSI (14D)', '20 Day CCI', 'Williams %R', 'MA50', 'FFR']]\n",
    "vif_ticker[\"feature\"] = features.columns\n",
    "\n",
    "vif_ticker[\"VIF\"] = [variance_inflation_factor(features.values, i)\n",
    "                          for i in range(len(features.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1 loss: 0.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\3942879950.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  losses = pd.concat([losses, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 loss: 0.6128\n",
      "Epoch 21 loss: 0.6502\n",
      "Epoch 31 loss: 0.5963\n",
      "Epoch 41 loss: 0.8048\n",
      "Epoch 51 loss: 0.5672\n",
      "Epoch 61 loss: 0.5264\n",
      "Epoch 71 loss: 0.6883\n",
      "Epoch 81 loss: 0.6441\n",
      "Epoch 91 loss: 0.6048\n",
      "Epoch 99 loss: 0.5355\n",
      "\n",
      "Accuracy: 0.5516\n",
      "Precision: 0.5917\n",
      "Recall: 0.5464\n",
      "F1: 0.5682\n",
      "Kappa: 0.1035\n",
      "\n",
      "Fold 2\n",
      "Epoch 1 loss: 0.6434\n",
      "Epoch 11 loss: 0.5994\n",
      "Epoch 21 loss: 0.6793\n",
      "Epoch 31 loss: 0.5463\n",
      "Epoch 41 loss: 0.6511\n",
      "Epoch 51 loss: 0.4734\n",
      "Epoch 61 loss: 0.7236\n",
      "Epoch 71 loss: 0.6281\n",
      "Epoch 81 loss: 0.6529\n",
      "Epoch 91 loss: 0.5020\n",
      "Epoch 99 loss: 0.5135\n",
      "\n",
      "Accuracy: 0.5310\n",
      "Precision: 0.5536\n",
      "Recall: 0.8421\n",
      "F1: 0.6681\n",
      "Kappa: -0.0255\n",
      "\n",
      "Fold 3\n",
      "Epoch 1 loss: 0.7450\n",
      "Epoch 11 loss: 0.6438\n",
      "Epoch 21 loss: 0.6439\n",
      "Epoch 31 loss: 0.6810\n",
      "Epoch 41 loss: 0.5997\n",
      "Epoch 51 loss: 0.6107\n",
      "Epoch 61 loss: 0.5938\n",
      "Epoch 71 loss: 0.6717\n",
      "Epoch 81 loss: 0.6251\n",
      "Epoch 91 loss: 0.6834\n",
      "Epoch 99 loss: 0.4348\n",
      "\n",
      "Accuracy: 0.5044\n",
      "Precision: 0.5827\n",
      "Recall: 0.3915\n",
      "F1: 0.4684\n",
      "Kappa: 0.0366\n",
      "\n",
      "Fold 4\n",
      "Epoch 1 loss: 0.7129\n",
      "Epoch 11 loss: 0.6960\n",
      "Epoch 21 loss: 0.6360\n",
      "Epoch 31 loss: 0.6481\n",
      "Epoch 41 loss: 0.6501\n",
      "Epoch 51 loss: 0.5483\n",
      "Epoch 61 loss: 0.6964\n",
      "Epoch 71 loss: 0.7196\n",
      "Epoch 81 loss: 0.5316\n",
      "Epoch 91 loss: 0.7975\n",
      "Epoch 99 loss: 0.4830\n",
      "\n",
      "Accuracy: 0.5015\n",
      "Precision: 0.4964\n",
      "Recall: 0.8204\n",
      "F1: 0.6185\n",
      "Kappa: 0.0121\n",
      "\n",
      "Fold 5\n",
      "Epoch 1 loss: 0.7007\n",
      "Epoch 11 loss: 0.6900\n",
      "Epoch 21 loss: 0.6188\n",
      "Epoch 31 loss: 0.6522\n",
      "Epoch 41 loss: 0.6373\n",
      "Epoch 51 loss: 0.5830\n",
      "Epoch 61 loss: 0.6638\n",
      "Epoch 71 loss: 0.7100\n",
      "Epoch 81 loss: 0.6749\n",
      "Epoch 91 loss: 0.6192\n",
      "Epoch 99 loss: 0.6947\n",
      "\n",
      "Accuracy: 0.4867\n",
      "Precision: 0.5461\n",
      "Recall: 0.4415\n",
      "F1: 0.4882\n",
      "Kappa: -0.0151\n",
      "\n",
      "Fold 6\n",
      "Epoch 1 loss: 0.6576\n",
      "Epoch 11 loss: 0.6524\n",
      "Epoch 21 loss: 0.6730\n",
      "Epoch 31 loss: 0.7108\n",
      "Epoch 41 loss: 0.6640\n",
      "Epoch 51 loss: 0.6921\n",
      "Epoch 61 loss: 0.6853\n",
      "Epoch 71 loss: 0.6699\n",
      "Epoch 81 loss: 0.7389\n",
      "Epoch 91 loss: 0.5363\n",
      "Epoch 99 loss: 0.6758\n",
      "\n",
      "Accuracy: 0.5251\n",
      "Precision: 0.5315\n",
      "Recall: 0.9725\n",
      "F1: 0.6874\n",
      "Kappa: -0.0226\n",
      "\n",
      "Fold 7\n",
      "Epoch 1 loss: 0.6385\n",
      "Epoch 11 loss: 0.6178\n",
      "Epoch 21 loss: 0.6839\n",
      "Epoch 31 loss: 0.6471\n",
      "Epoch 41 loss: 0.6863\n",
      "Epoch 51 loss: 0.6560\n",
      "Epoch 61 loss: 0.6677\n",
      "Epoch 71 loss: 0.5439\n",
      "Epoch 81 loss: 0.6264\n",
      "Epoch 91 loss: 0.5536\n",
      "Epoch 99 loss: 0.7188\n",
      "\n",
      "Accuracy: 0.5103\n",
      "Precision: 0.5781\n",
      "Recall: 0.5663\n",
      "F1: 0.5722\n",
      "Kappa: -0.0001\n",
      "\n",
      "Fold 8\n",
      "Epoch 1 loss: 0.7042\n",
      "Epoch 11 loss: 0.6744\n",
      "Epoch 21 loss: 0.6696\n",
      "Epoch 31 loss: 0.6676\n",
      "Epoch 41 loss: 0.7126\n",
      "Epoch 51 loss: 0.6366\n",
      "Epoch 61 loss: 0.6475\n",
      "Epoch 71 loss: 0.7316\n",
      "Epoch 81 loss: 0.6958\n",
      "Epoch 91 loss: 0.6794\n",
      "Epoch 99 loss: 0.7447\n",
      "\n",
      "Accuracy: 0.5811\n",
      "Precision: 0.5811\n",
      "Recall: 1.0000\n",
      "F1: 0.7351\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 9\n",
      "Epoch 1 loss: 0.7080\n",
      "Epoch 11 loss: 0.6950\n",
      "Epoch 21 loss: 0.6883\n",
      "Epoch 31 loss: 0.7576\n",
      "Epoch 41 loss: 0.6809\n",
      "Epoch 51 loss: 0.6531\n",
      "Epoch 61 loss: 0.6546\n",
      "Epoch 71 loss: 0.6770\n",
      "Epoch 81 loss: 0.6826\n",
      "Epoch 91 loss: 0.7180\n",
      "Epoch 99 loss: 0.7404\n",
      "\n",
      "Accuracy: 0.4808\n",
      "Precision: 0.4429\n",
      "Recall: 0.6118\n",
      "F1: 0.5138\n",
      "Kappa: -0.0133\n",
      "\n",
      "Fold 10\n",
      "Epoch 1 loss: 0.6846\n",
      "Epoch 11 loss: 0.6904\n",
      "Epoch 21 loss: 0.6990\n",
      "Epoch 31 loss: 0.7173\n",
      "Epoch 41 loss: 0.6878\n",
      "Epoch 51 loss: 0.6245\n",
      "Epoch 61 loss: 0.6706\n",
      "Epoch 71 loss: 0.6946\n",
      "Epoch 81 loss: 0.6848\n",
      "Epoch 91 loss: 0.6366\n",
      "Epoch 99 loss: 0.7113\n",
      "\n",
      "Accuracy: 0.4366\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1: 0.0000\n",
      "Kappa: 0.0000\n",
      "\n",
      "\n",
      "Cross-Validation Results:\n",
      "Average Accuracy: 0.5109\n",
      "Average Precision: 0.4904\n",
      "Average Recall: 0.6193\n",
      "Average F1: 0.5320\n",
      "Average Kappa: 0.0076\n"
     ]
    }
   ],
   "source": [
    "# create LSTM model class\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer, num_layers=4, dropout=0.3):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers with dropout\n",
    "        self.lstm = nn.LSTM(input_layer, hidden_layer, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.linear_layer = nn.Linear(hidden_layer, output_layer)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, input_tensor.size(0), self.hidden_layer).to(input_tensor.device)\n",
    "        c0 = torch.zeros(self.num_layers, input_tensor.size(0), self.hidden_layer).to(input_tensor.device)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        out, _ = self.lstm(input_tensor, (h0, c0))\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        out = self.linear_layer(out)\n",
    "        return out\n",
    "\n",
    "# create sequences for input data and corresponding labels\n",
    "def create_sequence(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(0, len(input_data) - sequence_length):\n",
    "        sequence = input_data[i : i + sequence_length, :-1]\n",
    "        label = input_data[i + sequence_length, -1]\n",
    "        sequences.append((sequence, label))\n",
    "    return sequences\n",
    "\n",
    "# train the model with data provided\n",
    "def trainer(model, train_data, loss_func, opt, epochs, fold, losses):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for sequence, labels in train_data:\n",
    "            opt.zero_grad()\n",
    "            sequence = sequence.clone().detach().float()\n",
    "            labels = labels.clone().detach().float().view(-1, 1)\n",
    "            \n",
    "            y = model(sequence)\n",
    "            loss = loss_func(y, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 10 == 1 or epoch == 99:\n",
    "            print(f'Epoch {epoch} loss: {loss.item():.4f}')\n",
    "\n",
    "            new_row = pd.DataFrame({'Fold': [fold+1], 'Epoch': [epoch], 'Loss': [loss.item()]})\n",
    "            losses = pd.concat([losses, new_row], ignore_index=True)\n",
    "            \n",
    "    return losses\n",
    "\n",
    "def predictor(model, test_data):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for sequence, _ in test_data:\n",
    "            sequence = sequence.clone().detach().float()\n",
    "            y = model(sequence)\n",
    "            batch_predictions = torch.sigmoid(y)\n",
    "\n",
    "            # Convert predictions to tensor if they are scalar or float\n",
    "            if isinstance(batch_predictions, float) or batch_predictions.ndimension() == 0:\n",
    "                batch_predictions = torch.tensor([batch_predictions])\n",
    "\n",
    "            batch_predictions = torch.round(batch_predictions)\n",
    "\n",
    "            predictions.extend(batch_predictions.squeeze().tolist())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# create sequences\n",
    "sequence_length = 10\n",
    "sequences = create_sequence(ticker[['Close', 'Volume', 'CPI', 'Mortgage_rate', 'Unemp_rate', \n",
    "                                    'disposable_income','GVZ', 'OVX', 'VVIX', 'RSI (14D)', \n",
    "                                    '20 Day CCI', 'Williams %R', 'MA50', 'FFR', 'Target']].values, sequence_length)\n",
    "\n",
    "# cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "kappas = []\n",
    "losses = pd.DataFrame(columns=['Fold', 'Epoch', 'Loss'])\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(sequences)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    train_sequences = [sequences[i] for i in train_index]\n",
    "    test_sequences = [sequences[i] for i in test_index]\n",
    "    \n",
    "    train_data = torch.utils.data.DataLoader(train_sequences, shuffle=True, batch_size=32)\n",
    "    test_data = torch.utils.data.DataLoader(test_sequences, shuffle=False, batch_size=32)\n",
    "    \n",
    "    # initialize model\n",
    "    model = LSTM_Model(input_layer=14, hidden_layer=50, output_layer=1)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    \n",
    "    # train model\n",
    "    epochs = 100\n",
    "    losses = trainer(model, train_data, loss_func, opt, epochs, fold, losses)\n",
    "    \n",
    "    # make predictions\n",
    "    test_labels = [label for _, label in test_sequences]\n",
    "    predictions = predictor(model, test_data)\n",
    "    \n",
    "    # calculate statistics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    kappa = cohen_kappa_score(test_labels, predictions)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    kappas.append(kappa)\n",
    "    \n",
    "    print(f'\\nAccuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1: {f1:.4f}')\n",
    "    print(f'Kappa: {kappa:.4f}\\n')\n",
    "\n",
    "# average scores across all folds\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1s)\n",
    "avg_kappa = np.mean(kappas)\n",
    "\n",
    "print(f'\\nCross-Validation Results:')\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n",
    "print(f'Average Recall: {avg_recall:.4f}')\n",
    "print(f'Average F1: {avg_f1:.4f}')\n",
    "print(f'Average Kappa: {avg_kappa:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days rising: 2036\n",
      "Number of days falling: 1703\n",
      "Rise % is: 54.45%\n",
      "Fall % is: 45.55%\n"
     ]
    }
   ],
   "source": [
    "fall = (ticker.Target == 0).sum()\n",
    "rise = (ticker.Target == 1).sum()\n",
    "\n",
    "rise_per = (rise / (rise + fall)) * 100\n",
    "fall_per = (fall / (rise + fall)) * 100\n",
    "\n",
    "print(f'Number of days rising: {rise}')\n",
    "print(f'Number of days falling: {fall}')\n",
    "\n",
    "print(f'Rise % is: {rise_per:.2f}%')\n",
    "print(f'Fall % is: {fall_per:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_24448\\4160461266.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  export_results = results._append(metrics, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F1</th>\n",
       "      <th>Average Kappa</th>\n",
       "      <th>Number of Rises</th>\n",
       "      <th>Number of Falls</th>\n",
       "      <th>Rise %</th>\n",
       "      <th>Fall %</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>^SPX</th>\n",
       "      <td>0.5109</td>\n",
       "      <td>0.4904</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>2036</td>\n",
       "      <td>1703</td>\n",
       "      <td>54.4531</td>\n",
       "      <td>45.5469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.6128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.6502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.5963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>0.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>71</td>\n",
       "      <td>0.6946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>81</td>\n",
       "      <td>0.6848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>91</td>\n",
       "      <td>0.6366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>99</td>\n",
       "      <td>0.7113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Average Accuracy  Average Precision  Average Recall  Average F1  \\\n",
       "Dataset                                                                    \n",
       "^SPX               0.5109             0.4904          0.6193      0.5320   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "...                   ...                ...             ...         ...   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "NaN                   NaN                NaN             NaN         NaN   \n",
       "\n",
       "         Average Kappa Number of Rises Number of Falls  Rise %  Fall % Fold  \\\n",
       "Dataset                                                                       \n",
       "^SPX            0.0076            2036            1703 54.4531 45.5469  NaN   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN    1   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN    1   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN    1   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN    1   \n",
       "...                ...             ...             ...     ...     ...  ...   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN   10   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN   10   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN   10   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN   10   \n",
       "NaN                NaN             NaN             NaN     NaN     NaN   10   \n",
       "\n",
       "        Epoch   Loss  \n",
       "Dataset               \n",
       "^SPX      NaN    NaN  \n",
       "NaN         1 0.6104  \n",
       "NaN        11 0.6128  \n",
       "NaN        21 0.6502  \n",
       "NaN        31 0.5963  \n",
       "...       ...    ...  \n",
       "NaN        61 0.6706  \n",
       "NaN        71 0.6946  \n",
       "NaN        81 0.6848  \n",
       "NaN        91 0.6366  \n",
       "NaN        99 0.7113  \n",
       "\n",
       "[111 rows x 12 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Dataset', 'Average Accuracy', 'Average Precision', \n",
    "                                'Average Recall', 'Average F1', 'Average Kappa', 'Number of Rises', \n",
    "                                'Number of Falls', 'Rise %', 'Fall %'])\n",
    "\n",
    "datatset_name = (file_path.split('/')[-1].split('.')[0])\n",
    "\n",
    "metrics = {'Dataset': datatset_name ,'Average Accuracy': avg_accuracy, \n",
    "           'Average Precision': avg_precision, 'Average Recall': avg_recall, \n",
    "           'Average F1': avg_f1, 'Average Kappa': avg_kappa, 'Number of Rises': rise, 'Number of Falls': fall, \n",
    "           'Rise %': rise_per, 'Fall %': fall_per}\n",
    "\n",
    "export_results = results._append(metrics, ignore_index=True)\n",
    "\n",
    "export_results = pd.concat([export_results, losses], ignore_index=True)\n",
    "\n",
    "export_results.set_index('Dataset', inplace=True)\n",
    "\n",
    "export_results.to_csv(r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\results\\\\\" + datatset_name + '_DAILY_LOSS.csv')\n",
    "\n",
    "export_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
