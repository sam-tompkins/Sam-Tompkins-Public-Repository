{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# get dataset\u001b[39;00m\n\u001b[0;32m    100\u001b[0m file_path \u001b[38;5;241m=\u001b[39m filedialog\u001b[38;5;241m.\u001b[39maskopenfilename(parent\u001b[38;5;241m=\u001b[39mroot, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect A File\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m ticker \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m    102\u001b[0m ticker, scaler \u001b[38;5;241m=\u001b[39m prep(ticker)\n\u001b[0;32m    104\u001b[0m ticker\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas_ta as ta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "root = tk.Tk()\n",
    "root.wm_attributes('-topmost', 1)\n",
    "root.withdraw()\n",
    "\n",
    "# feature engineering and scaling\n",
    "def prep(dataset):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    macro_path1 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\IRP DATASET  - Copy of US_macroeconomics.csv\"\n",
    "    macro_path2 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\^VIX.csv\"\n",
    "    macro_path3 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\CORESTICKM159SFRBATL.csv\"\n",
    "    macro_path4 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\FFR.csv\"\n",
    "    macro_path5 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\GDP.csv\"\n",
    "    macro_path6 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\VIX9D_History.csv\"\n",
    "    macro_path7 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\GVZ_History.csv\"\n",
    "    macro_path8 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\OVX_History.csv\"\n",
    "    macro_path9 = r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\raw_data\\VVIX_History.csv\"\n",
    "    macro_data1 = pd.read_csv(macro_path1)\n",
    "    macro_data2 = pd.read_csv(macro_path2)\n",
    "    macro_data3 = pd.read_csv(macro_path3)\n",
    "    macro_data4 = pd.read_csv(macro_path4)\n",
    "    macro_data5 = pd.read_csv(macro_path5)\n",
    "    macro_data6 = pd.read_csv(macro_path6)\n",
    "    macro_data7 = pd.read_csv(macro_path7)\n",
    "    macro_data8 = pd.read_csv(macro_path8)\n",
    "    macro_data9 = pd.read_csv(macro_path9)\n",
    "\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'], dayfirst=True)\n",
    "    dataset.set_index('Date', inplace=True)\n",
    "    dataset['Target'] = np.where(dataset['Close'].shift(-1) > dataset['Close'], 1, 0)\n",
    "\n",
    "    macro_data1['date'] = pd.to_datetime(macro_data1['date'], dayfirst=True)\n",
    "    macro_data1.set_index('date', inplace=True)\n",
    "\n",
    "    macro_data2['Date'] = pd.to_datetime(macro_data2['Date'], dayfirst=True)\n",
    "    macro_data2.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data3['Date'] = pd.to_datetime(macro_data3['Date'], dayfirst=True)\n",
    "    macro_data3.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data4['Date'] = pd.to_datetime(macro_data4['Date'], dayfirst=True)\n",
    "    macro_data4.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data5['Date'] = pd.to_datetime(macro_data5['Date'], dayfirst=True)\n",
    "    macro_data5.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data7['Date'] = pd.to_datetime(macro_data7['Date'], dayfirst=False)\n",
    "    macro_data7.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data8['Date'] = pd.to_datetime(macro_data8['Date'], dayfirst=False)\n",
    "    macro_data8.set_index('Date', inplace=True)\n",
    "\n",
    "    macro_data9['Date'] = pd.to_datetime(macro_data9['Date'])\n",
    "    macro_data9.set_index('Date', inplace=True)\n",
    "\n",
    "    merge_data1 = dataset.join(macro_data1, how='left').fillna(method='ffill')\n",
    "    merge_data2 = merge_data1.join(macro_data2, how='left').fillna(method='ffill')\n",
    "    merge_data3 = merge_data2.join(macro_data3, how='left').fillna(method='ffill')\n",
    "    merge_data4 = merge_data3.join(macro_data4, how='left').fillna(method='ffill')\n",
    "    merge_data5 = merge_data4.join(macro_data5, how='left').fillna(method='ffill')\n",
    "    merge_data7 = merge_data5.join(macro_data7, how='left').fillna(method='ffill')\n",
    "    merge_data8 = merge_data7.join(macro_data8, how='left').fillna(method='ffill')\n",
    "    merge_final = merge_data8.join(macro_data9, how='left').fillna(method='ffill')\n",
    "\n",
    "    merge_final['RSI (14D)'] = ta.rsi(merge_final['Close'], length=14)\n",
    "    merge_final['20 Day CCI'] = ta.cci(high=merge_final['High'], low=merge_final['Low'], \n",
    "                                    close=merge_final['Close'], length=20)\n",
    "    merge_final['Williams %R'] = ta.willr(high=merge_final['High'], low=merge_final['Low'], \n",
    "                                        close=merge_final['Close'], length=14)\n",
    "    merge_final['EMA (5D)'] = merge_final['Close'].ewm(span=5, adjust=False).mean()\n",
    "    merge_final['MA50'] = merge_final['Close'].rolling(window=50).mean()\n",
    "\n",
    "    final_dataset = merge_final.dropna()\n",
    "\n",
    "    time_shifted = final_dataset.resample('M').asfreq().dropna().head(102)\n",
    "    \n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'CPI', \n",
    "                'Mortgage_rate', 'Unemp_rate', 'disposable_income','GVZ', 'OVX', 'VVIX', \n",
    "                'RSI (14D)', '20 Day CCI', 'Williams %R', 'EMA (5D)', 'MA50', 'DFF']\n",
    "    \n",
    "    time_shifted[features] = time_shifted[features].astype(float)\n",
    "    time_shifted[features] = scaler.fit_transform(time_shifted[features])\n",
    "\n",
    "    return time_shifted, scaler\n",
    "\n",
    "# get dataset\n",
    "file_path = filedialog.askopenfilename(parent=root, title=\"Select A File\")\n",
    "ticker = pd.read_csv(file_path)\n",
    "ticker, scaler = prep(ticker)\n",
    "\n",
    "ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1 loss: 0.7066\n",
      "Epoch 26 loss: 0.6957\n",
      "Epoch 51 loss: 0.7025\n",
      "Epoch 76 loss: 0.6857\n",
      "Epoch 101 loss: 0.6857\n",
      "Epoch 126 loss: 0.6680\n",
      "\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.5000\n",
      "Recall: 1.0000\n",
      "F1: 0.6667\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 2\n",
      "Epoch 1 loss: 0.6988\n",
      "Epoch 26 loss: 0.6935\n",
      "Epoch 51 loss: 0.6884\n",
      "Epoch 76 loss: 0.6760\n",
      "Epoch 101 loss: 0.6556\n",
      "Epoch 126 loss: 0.6655\n",
      "\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.5000\n",
      "Recall: 1.0000\n",
      "F1: 0.6667\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 3\n",
      "Epoch 1 loss: 0.6943\n",
      "Epoch 26 loss: 0.6954\n",
      "Epoch 51 loss: 0.6943\n",
      "Epoch 76 loss: 0.6951\n",
      "Epoch 101 loss: 0.6798\n",
      "Epoch 126 loss: 0.6715\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1: 0.0000\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 4\n",
      "Epoch 1 loss: 0.6890\n",
      "Epoch 26 loss: 0.6872\n",
      "Epoch 51 loss: 0.7061\n",
      "Epoch 76 loss: 0.7206\n",
      "Epoch 101 loss: 0.7258\n",
      "Epoch 126 loss: 0.6038\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.6667\n",
      "Recall: 0.5000\n",
      "F1: 0.5714\n",
      "Kappa: 0.2500\n",
      "\n",
      "Fold 5\n",
      "Epoch 1 loss: 0.7085\n",
      "Epoch 26 loss: 0.7060\n",
      "Epoch 51 loss: 0.7083\n",
      "Epoch 76 loss: 0.6857\n",
      "Epoch 101 loss: 0.6857\n",
      "Epoch 126 loss: 0.6317\n",
      "\n",
      "Accuracy: 0.7500\n",
      "Precision: 0.7500\n",
      "Recall: 1.0000\n",
      "F1: 0.8571\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 6\n",
      "Epoch 1 loss: 0.6965\n",
      "Epoch 26 loss: 0.6891\n",
      "Epoch 51 loss: 0.6828\n",
      "Epoch 76 loss: 0.6712\n",
      "Epoch 101 loss: 0.4296\n",
      "Epoch 126 loss: 0.6661\n",
      "\n",
      "Accuracy: 0.7500\n",
      "Precision: 1.0000\n",
      "Recall: 0.6667\n",
      "F1: 0.8000\n",
      "Kappa: 0.5000\n",
      "\n",
      "Fold 7\n",
      "Epoch 1 loss: 0.7073\n",
      "Epoch 26 loss: 0.6938\n",
      "Epoch 51 loss: 0.6643\n",
      "Epoch 76 loss: 0.5880\n",
      "Epoch 101 loss: 0.6496\n",
      "Epoch 126 loss: 0.5696\n",
      "\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1: 0.0000\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 8\n",
      "Epoch 1 loss: 0.6488\n",
      "Epoch 26 loss: 0.6637\n",
      "Epoch 51 loss: 0.6832\n",
      "Epoch 76 loss: 0.7496\n",
      "Epoch 101 loss: 0.6170\n",
      "Epoch 126 loss: 0.7084\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.6250\n",
      "Recall: 1.0000\n",
      "F1: 0.7692\n",
      "Kappa: 0.0000\n",
      "\n",
      "Fold 9\n",
      "Epoch 1 loss: 0.6804\n",
      "Epoch 26 loss: 0.7023\n",
      "Epoch 51 loss: 0.6698\n",
      "Epoch 76 loss: 0.6608\n",
      "Epoch 101 loss: 0.5381\n",
      "Epoch 126 loss: 0.5545\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.6667\n",
      "Recall: 0.8000\n",
      "F1: 0.7273\n",
      "Kappa: 0.1429\n",
      "\n",
      "Fold 10\n",
      "Epoch 1 loss: 0.7226\n",
      "Epoch 26 loss: 0.7154\n",
      "Epoch 51 loss: 0.6533\n",
      "Epoch 76 loss: 0.5486\n",
      "Epoch 101 loss: 0.5585\n",
      "Epoch 126 loss: 0.5300\n",
      "\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.6250\n",
      "Recall: 1.0000\n",
      "F1: 0.7692\n",
      "Kappa: 0.0000\n",
      "\n",
      "\n",
      "Cross-Validation Results:\n",
      "Average Accuracy: 0.6125\n",
      "Average Precision: 0.5333\n",
      "Average Recall: 0.6967\n",
      "Average F1: 0.5828\n",
      "Average Kappa: 0.0893\n"
     ]
    }
   ],
   "source": [
    "# create LSTM model class\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer, num_layers=4, dropout=0.3):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers with dropout\n",
    "        self.lstm = nn.LSTM(input_layer, hidden_layer, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.linear_layer = nn.Linear(hidden_layer, output_layer)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, input_tensor.size(0), self.hidden_layer).to(input_tensor.device)\n",
    "        c0 = torch.zeros(self.num_layers, input_tensor.size(0), self.hidden_layer).to(input_tensor.device)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        out, _ = self.lstm(input_tensor, (h0, c0))\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        out = self.linear_layer(out)\n",
    "        return out\n",
    "\n",
    "# create sequences for input data and corresponding labels\n",
    "def create_sequence(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(0, len(input_data) - sequence_length):\n",
    "        sequence = input_data[i : i + sequence_length, :-1]\n",
    "        label = input_data[i + sequence_length, -1]\n",
    "        sequences.append((sequence, label))\n",
    "    return sequences\n",
    "\n",
    "# train the model with data provided\n",
    "def trainer(model, train_data, loss_func, opt, epochs, fold, losses):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for sequence, labels in train_data:\n",
    "            opt.zero_grad()\n",
    "            sequence = sequence.clone().detach().float()\n",
    "            labels = labels.clone().detach().float().view(-1, 1)\n",
    "            \n",
    "            y = model(sequence)\n",
    "            loss = loss_func(y, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch % 25 == 1:\n",
    "            print(f'Epoch {epoch} loss: {loss.item():.4f}')\n",
    "\n",
    "            new_row = pd.DataFrame({'Fold': [fold+1], 'Epoch': [epoch], 'Loss': [loss.item()]})\n",
    "            losses = pd.concat([losses, new_row], ignore_index=True)\n",
    "            \n",
    "    return losses\n",
    "\n",
    "def predictor(model, test_data):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for sequence, _ in test_data:\n",
    "            sequence = sequence.clone().detach().float()\n",
    "            y = model(sequence)\n",
    "            batch_predictions = torch.sigmoid(y)\n",
    "\n",
    "            # Convert predictions to tensor if they are scalar or float\n",
    "            if isinstance(batch_predictions, float) or batch_predictions.ndimension() == 0:\n",
    "                batch_predictions = torch.tensor([batch_predictions])\n",
    "\n",
    "            batch_predictions = torch.round(batch_predictions)\n",
    "\n",
    "            predictions.extend(batch_predictions.squeeze().tolist())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# create sequences\n",
    "sequence_length = 10\n",
    "sequences = create_sequence(ticker[['Close', 'Volume', 'CPI', 'Mortgage_rate', 'disposable_income',\n",
    "                                    'GVZ', 'OVX', 'VVIX', 'RSI (14D)', '20 Day CCI', 'Williams %R', \n",
    "                                    'MA50', 'Target']].values, sequence_length)\n",
    "\n",
    "# cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "kappas = []\n",
    "losses = pd.DataFrame(columns=['Fold', 'Epoch', 'Loss'])\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(sequences)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    train_sequences = [sequences[i] for i in train_index]\n",
    "    test_sequences = [sequences[i] for i in test_index]\n",
    "    \n",
    "    train_data = torch.utils.data.DataLoader(train_sequences, shuffle=True, batch_size=32)\n",
    "    test_data = torch.utils.data.DataLoader(test_sequences, shuffle=False, batch_size=32)\n",
    "    \n",
    "    # initialize model\n",
    "    model = LSTM_Model(input_layer=12, hidden_layer=4, output_layer=1)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    \n",
    "    # train model\n",
    "    epochs = 100\n",
    "    losses = trainer(model, train_data, loss_func, opt, epochs, fold, losses)\n",
    "    \n",
    "    # make predictions\n",
    "    test_labels = [label for _, label in test_sequences]\n",
    "    predictions = predictor(model, test_data)\n",
    "    \n",
    "    # calculate statistics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    kappa = cohen_kappa_score(test_labels, predictions)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    kappas.append(kappa)\n",
    "    \n",
    "    print(f'\\nAccuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1: {f1:.4f}')\n",
    "    print(f'Kappa: {kappa:.4f}\\n')\n",
    "\n",
    "# average scores across all folds\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1s)\n",
    "avg_kappa = np.mean(kappas)\n",
    "\n",
    "print(f'\\nCross-Validation Results:')\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n",
    "print(f'Average Recall: {avg_recall:.4f}')\n",
    "print(f'Average F1: {avg_f1:.4f}')\n",
    "print(f'Average Kappa: {avg_kappa:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days rising: 60\n",
      "Number of days falling: 42\n",
      "Rise % is: 58.82%\n",
      "Fall % is: 41.18%\n"
     ]
    }
   ],
   "source": [
    "fall = (ticker.Target == 0).sum()\n",
    "rise = (ticker.Target == 1).sum()\n",
    "\n",
    "rise_per = (rise / (rise + fall)) * 100\n",
    "fall_per = (fall / (rise + fall)) * 100\n",
    "\n",
    "print(f'Number of days rising: {rise}')\n",
    "print(f'Number of days falling: {fall}')\n",
    "\n",
    "print(f'Rise % is: {rise_per:.2f}%')\n",
    "print(f'Fall % is: {fall_per:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F1</th>\n",
       "      <th>Average Kappa</th>\n",
       "      <th>Number of Rises</th>\n",
       "      <th>Number of Falls</th>\n",
       "      <th>Rise %</th>\n",
       "      <th>Fall %</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>^DJI_daily</th>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>60</td>\n",
       "      <td>42</td>\n",
       "      <td>58.823529</td>\n",
       "      <td>41.176471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.695672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0.702525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>0.685694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.715403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>0.653261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>76</td>\n",
       "      <td>0.548608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>0.558491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>126</td>\n",
       "      <td>0.529963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Average Accuracy  Average Precision  Average Recall  Average F1  \\\n",
       "Dataset                                                                       \n",
       "^DJI_daily            0.6125           0.533333        0.696667    0.582764   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "...                      ...                ...             ...         ...   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "NaN                      NaN                NaN             NaN         NaN   \n",
       "\n",
       "            Average Kappa Number of Rises Number of Falls     Rise %  \\\n",
       "Dataset                                                                \n",
       "^DJI_daily       0.089286              60              42  58.823529   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "...                   ...             ...             ...        ...   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "NaN                   NaN             NaN             NaN        NaN   \n",
       "\n",
       "               Fall % Fold Epoch      Loss  \n",
       "Dataset                                     \n",
       "^DJI_daily  41.176471  NaN   NaN       NaN  \n",
       "NaN               NaN    1     1  0.706595  \n",
       "NaN               NaN    1    26  0.695672  \n",
       "NaN               NaN    1    51  0.702525  \n",
       "NaN               NaN    1    76  0.685694  \n",
       "...               ...  ...   ...       ...  \n",
       "NaN               NaN   10    26  0.715403  \n",
       "NaN               NaN   10    51  0.653261  \n",
       "NaN               NaN   10    76  0.548608  \n",
       "NaN               NaN   10   101  0.558491  \n",
       "NaN               NaN   10   126  0.529963  \n",
       "\n",
       "[61 rows x 12 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Dataset', 'Average Accuracy', 'Average Precision', \n",
    "                                'Average Recall', 'Average F1', 'Average Kappa', 'Number of Rises', \n",
    "                                'Number of Falls', 'Rise %', 'Fall %'])\n",
    "\n",
    "datatset_name = (file_path.split('/')[-1].split('.')[0]) + '_daily'\n",
    "\n",
    "metrics = {'Dataset': datatset_name ,'Average Accuracy': avg_accuracy, \n",
    "           'Average Precision': avg_precision, 'Average Recall': avg_recall, \n",
    "           'Average F1': avg_f1, 'Average Kappa': avg_kappa, 'Number of Rises': rise, 'Number of Falls': fall, \n",
    "           'Rise %': rise_per, 'Fall %': fall_per}\n",
    "\n",
    "export_results = results._append(metrics, ignore_index=True)\n",
    "\n",
    "export_results = pd.concat([export_results, losses], ignore_index=True)\n",
    "\n",
    "export_results.set_index('Dataset', inplace=True)\n",
    "\n",
    "export_results.to_csv(r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\IRP DATA\\\\\" + datatset_name + '_daily_w_macro_NEWWWerere.csv')\n",
    "\n",
    "export_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
