{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas_ta as ta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "root = tk.Tk()\n",
    "root.wm_attributes('-topmost', 1)\n",
    "root.withdraw()\n",
    "\n",
    "# model, data prep, and model run\n",
    "\n",
    "# normalize data and assign movement direction values\n",
    "def prep(dataset):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    dataset['RSI (14D)'] = ta.rsi(dataset['Close'], length=14)\n",
    "    dataset['20 Day CCI'] = ta.cci(high=dataset['High'], low=dataset['Low'], \n",
    "                                    close=dataset['Close'], length=20)\n",
    "    dataset['Williams %R'] = ta.willr(high=dataset['High'], low=dataset['Low'], \n",
    "                                        close=dataset['Close'], length=14)\n",
    "    dataset['EMA (5D)'] = dataset['Close'].ewm(span=5, adjust=False).mean()\n",
    "\n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Volume', 'RSI (14D)', \n",
    "                '20 Day CCI', 'Williams %R', 'Mortgage_rate', 'Unemp_rate',\n",
    "                'disposable_income', 'Personal_consumption_expenditure', \n",
    "                'personal_savings', 'Open_VIX', 'Close_VIX', \n",
    "                'Adj Close_VIX',\t'CORESTICKM159SFRBATL',\t'DFF',\t'GDP']\n",
    "    \n",
    "    dataset[features] = dataset[features].astype(float)\n",
    "    dataset[features] = scaler.fit_transform(dataset[features])\n",
    "\n",
    "    dataset['MA50'] = dataset['Close'].rolling(window=50).mean()\n",
    "\n",
    "    dataset['Target'] = np.where(dataset['Close'].shift(-1) > dataset['Close'], 1, 0)\n",
    "    prepared_data = dataset.dropna().tail(503)\n",
    "\n",
    "    return prepared_data, scaler\n",
    "\n",
    "# create LSTM model class\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.lstm = nn.LSTM(input_layer, hidden_layer, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(hidden_layer, output_layer)\n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_layer),\n",
    "                            torch.zeros(1, 1, self.hidden_layer))\n",
    "\n",
    "    # Define the forward pass of the LSTM_Model\n",
    "    def forward(self, input_tensor):\n",
    "        self.hidden_cell = (torch.zeros(1, input_tensor.size(0), self.hidden_layer), \n",
    "                            torch.zeros(1, input_tensor.size(0), self.hidden_layer))\n",
    "        \n",
    "        # Pass the input through the LSTM layer\n",
    "        out, self.hidden_cell = self.lstm(input_tensor, self.hidden_cell)\n",
    "        \n",
    "        # Get the output of the last time step\n",
    "        lstm_out_last = out[:, -1, :]\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        linear_out = self.linear_layer(lstm_out_last)\n",
    "        \n",
    "        return linear_out\n",
    "\n",
    "# create sequences for input data and corresponding labels\n",
    "def create_sequence(input_data, sequence_length):\n",
    "  sequences = []\n",
    "  for i in range(len(input_data) - sequence_length):\n",
    "    sequence = input_data[i : i + sequence_length, :-1]\n",
    "    label = input_data[i + sequence_length, -1]\n",
    "    sequences.append((sequence, label))\n",
    "  return sequences\n",
    "\n",
    "# train the model with data provided\n",
    "def trainer(model, train_data, loss_func, opt, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    for sequence, labels, in train_data:\n",
    "      opt.zero_grad()\n",
    "      model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer),\n",
    "                           torch.zeros(1, 1, model.hidden_layer))\n",
    "      \n",
    "      sequence = torch.tensor(sequence).float()\n",
    "      labels = torch.tensor(labels).float().view(-1, 1)\n",
    "\n",
    "      # Initialize the hidden state at the start of each sequence\n",
    "      model.hidden_cell = (torch.zeros(1, sequence.size(0), \n",
    "                                       model.hidden_layer),\n",
    "                            torch.zeros(1, sequence.size(0), \n",
    "                                        model.hidden_layer))\n",
    "\n",
    "      y = model(sequence)\n",
    "      loss = loss_func(y, labels)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "    # print progress as the model runs\n",
    "    if epoch % 25 == 1:\n",
    "      print(f'Epoch {epoch} loss: {loss.item():.4f}')\n",
    "\n",
    "# make predictions using trained model\n",
    "def predictor(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for sequence, _ in test_data:\n",
    "            sequence = torch.tensor(sequence).float()\n",
    "\n",
    "            # Initialize the hidden state at the start of each sequence\n",
    "            model.hidden_cell = (torch.zeros(1, sequence.size(0), \n",
    "                                             model.hidden_layer),\n",
    "                                 torch.zeros(1, sequence.size(0), \n",
    "                                             model.hidden_layer))\n",
    "            \n",
    "            y = model(sequence)\n",
    "\n",
    "            batch_predictions = torch.round(torch.sigmoid(y))\n",
    "\n",
    "            predictions.extend(batch_predictions.squeeze().tolist())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# load and prep data\n",
    "\n",
    "# get dataset\n",
    "file_path = filedialog.askopenfilename(parent=root, title=\"Select A File\")\n",
    "ticker = pd.read_csv(file_path)\n",
    "ticker, scaler = prep(ticker)\n",
    "\n",
    "# create sequences\n",
    "sequence_length = 10\n",
    "sequences = create_sequence(ticker[['Open', 'High', 'Low', 'Close', 'RSI (14D)', \n",
    "                '20 Day CCI', 'Williams %R', 'Mortgage_rate', 'Unemp_rate',\n",
    "                'disposable_income', 'Personal_consumption_expenditure', \n",
    "                'personal_savings', 'CORESTICKM159SFRBATL',\t\n",
    "                'DFF',\t'GDP', 'MA50', 'Target']].values, sequence_length)\n",
    "\n",
    "# cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=50)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "kappas = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(sequences)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    train_sequences = [sequences[i] for i in train_index]\n",
    "    test_sequences = [sequences[i] for i in test_index]\n",
    "    \n",
    "    train_data = torch.utils.data.DataLoader(train_sequences, shuffle=True, batch_size=8)\n",
    "    test_data = torch.utils.data.DataLoader(test_sequences, shuffle=True, batch_size=8)\n",
    "    \n",
    "    # initialize model\n",
    "    model = LSTM_Model(input_layer=16, hidden_layer=10, output_layer=1)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # train model\n",
    "    epochs = 150\n",
    "    trainer(model, train_data, loss_func, opt, epochs)\n",
    "    \n",
    "    # make predictions\n",
    "    test_labels = [label for _, label in test_sequences]\n",
    "    predictions = predictor(model, test_data)\n",
    "    \n",
    "    # calculate statistics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    kappa = cohen_kappa_score(test_labels, predictions)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    kappas.append(kappa)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1: {f1:.4f}')\n",
    "    print(f'Kappa: {kappa:.4f}')\n",
    "\n",
    "# average scores across all folds\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1s)\n",
    "avg_kappa = np.mean(kappas)\n",
    "\n",
    "print(f'\\nCross-Validation Results:')\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n",
    "print(f'Average Recall: {avg_recall:.4f}')\n",
    "print(f'Average F1: {avg_f1:.4f}')\n",
    "print(f'Average Kappa: {avg_kappa:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating importance for feature: Open\n",
      "Calculating importance for feature: High\n",
      "Calculating importance for feature: Low\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating importance for feature: Close\n",
      "Calculating importance for feature: Mortgage_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating importance for feature: Unemp_rate\n",
      "Calculating importance for feature: disposable_income\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating importance for feature: Personal_consumption_expenditure\n",
      "Calculating importance for feature: personal_savings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating importance for feature: DFF\n",
      "Calculating importance for feature: GDP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n",
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating importance for feature: MA50\n",
      "Feature: Open\n",
      "  Accuracy Drop: -0.2337\n",
      "  Precision Drop: -0.1439\n",
      "  Recall Drop: -0.3255\n",
      "  F1 Drop: -0.2439\n",
      "  Kappa Drop: -0.4101\n",
      "Feature: High\n",
      "  Accuracy Drop: -0.2215\n",
      "  Precision Drop: -0.1257\n",
      "  Recall Drop: -0.3352\n",
      "  F1 Drop: -0.2388\n",
      "  Kappa Drop: -0.3753\n",
      "Feature: Low\n",
      "  Accuracy Drop: -0.1912\n",
      "  Precision Drop: -0.1094\n",
      "  Recall Drop: -0.2964\n",
      "  F1 Drop: -0.2119\n",
      "  Kappa Drop: -0.3159\n",
      "Feature: Close\n",
      "  Accuracy Drop: -0.2215\n",
      "  Precision Drop: -0.1301\n",
      "  Recall Drop: -0.3255\n",
      "  F1 Drop: -0.2366\n",
      "  Kappa Drop: -0.3796\n",
      "Feature: Mortgage_rate\n",
      "  Accuracy Drop: -0.1791\n",
      "  Precision Drop: -0.0855\n",
      "  Recall Drop: -0.3255\n",
      "  F1 Drop: -0.2121\n",
      "  Kappa Drop: -0.2697\n",
      "Feature: Unemp_rate\n",
      "  Accuracy Drop: -0.1185\n",
      "  Precision Drop: -0.0427\n",
      "  Recall Drop: -0.2770\n",
      "  F1 Drop: -0.1667\n",
      "  Kappa Drop: -0.1344\n",
      "Feature: disposable_income\n",
      "  Accuracy Drop: -0.2276\n",
      "  Precision Drop: -0.1369\n",
      "  Recall Drop: -0.3255\n",
      "  F1 Drop: -0.2403\n",
      "  Kappa Drop: -0.3949\n",
      "Feature: Personal_consumption_expenditure\n",
      "  Accuracy Drop: -0.2397\n",
      "  Precision Drop: -0.1368\n",
      "  Recall Drop: -0.3547\n",
      "  F1 Drop: -0.2537\n",
      "  Kappa Drop: -0.4132\n",
      "Feature: personal_savings\n",
      "  Accuracy Drop: -0.2337\n",
      "  Precision Drop: -0.1392\n",
      "  Recall Drop: -0.3352\n",
      "  F1 Drop: -0.2460\n",
      "  Kappa Drop: -0.4060\n",
      "Feature: DFF\n",
      "  Accuracy Drop: -0.1488\n",
      "  Precision Drop: -0.0905\n",
      "  Recall Drop: -0.2284\n",
      "  F1 Drop: -0.1691\n",
      "  Kappa Drop: -0.2416\n",
      "Feature: GDP\n",
      "  Accuracy Drop: -0.2094\n",
      "  Precision Drop: -0.1210\n",
      "  Recall Drop: -0.3158\n",
      "  F1 Drop: -0.2272\n",
      "  Kappa Drop: -0.3532\n",
      "Feature: MA50\n",
      "  Accuracy Drop: -0.2519\n",
      "  Precision Drop: -0.1457\n",
      "  Recall Drop: -0.3644\n",
      "  F1 Drop: -0.2630\n",
      "  Kappa Drop: -0.4399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samto\\AppData\\Local\\Temp\\ipykernel_10888\\2402825896.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(sequence).float()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "\n",
    "# Function to calculate model performance\n",
    "def evaluate_model(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for sequence, label in test_data:\n",
    "            sequence = torch.tensor(sequence).float()\n",
    "            model.hidden_cell = (torch.zeros(1, sequence.size(0), model.hidden_layer),\n",
    "                                 torch.zeros(1, sequence.size(0), model.hidden_layer))\n",
    "            y = model(sequence)\n",
    "            batch_predictions = torch.round(torch.sigmoid(y))\n",
    "            predictions.extend(batch_predictions.squeeze().tolist())\n",
    "            true_labels.extend(label.tolist())\n",
    "        \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    kappa = cohen_kappa_score(true_labels, predictions)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, kappa\n",
    "\n",
    "# Calculate baseline performance\n",
    "baseline_accuracy, baseline_precision, baseline_recall, baseline_f1, baseline_kappa = evaluate_model(model, test_data)\n",
    "\n",
    "# Function to compute permutation importance for a specific feature\n",
    "def permutation_importance(model, test_data, feature_index, original_data):\n",
    "    permuted_data = original_data.copy()\n",
    "    permuted_data[:, feature_index] = np.random.permutation(permuted_data[:, feature_index])\n",
    "    permuted_sequences = create_sequence(permuted_data, sequence_length)\n",
    "    permuted_test_data = torch.utils.data.DataLoader(permuted_sequences, shuffle=True, batch_size=32)\n",
    "    \n",
    "    permuted_accuracy, permuted_precision, permuted_recall, permuted_f1, permuted_kappa = evaluate_model(model, permuted_test_data)\n",
    "    \n",
    "    accuracy_drop = baseline_accuracy - permuted_accuracy\n",
    "    precision_drop = baseline_precision - permuted_precision\n",
    "    recall_drop = baseline_recall - permuted_recall\n",
    "    f1_drop = baseline_f1 - permuted_f1\n",
    "    kappa_drop = baseline_kappa - permuted_kappa\n",
    "    \n",
    "    return accuracy_drop, precision_drop, recall_drop, f1_drop, kappa_drop\n",
    "\n",
    "# Get feature indices from the original dataset\n",
    "feature_names = ['Open', 'High', 'Low', 'Close', 'RSI (14D)', \n",
    "                 '20 Day CCI', 'Williams %R', 'Mortgage_rate', 'Unemp_rate',\n",
    "                 'disposable_income', 'Personal_consumption_expenditure', \n",
    "                 'personal_savings', 'CORESTICKM159SFRBATL',\t\n",
    "                 'DFF', 'GDP', 'MA50']\n",
    "feature_importance = {}\n",
    "\n",
    "# Calculate permutation importance for each feature\n",
    "for i, feature in enumerate(feature_names):\n",
    "    print(f'Calculating importance for feature: {feature}')\n",
    "    accuracy_drop, precision_drop, recall_drop, f1_drop, kappa_drop = permutation_importance(model, test_data, i, ticker[['Open', 'High', 'Low', 'Close', 'RSI (14D)', \n",
    "                '20 Day CCI', 'Williams %R', 'Mortgage_rate', 'Unemp_rate',\n",
    "                'disposable_income', 'Personal_consumption_expenditure', \n",
    "                'personal_savings', 'CORESTICKM159SFRBATL',\t\n",
    "                'DFF', 'GDP', 'MA50', 'Target']].values)\n",
    "    \n",
    "    feature_importance[feature] = {\n",
    "        'accuracy_drop': accuracy_drop,\n",
    "        'precision_drop': precision_drop,\n",
    "        'recall_drop': recall_drop,\n",
    "        'f1_drop': f1_drop,\n",
    "        'kappa_drop': kappa_drop\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for feature, importance in feature_importance.items():\n",
    "    print(f'Feature: {feature}')\n",
    "    print(f'  Accuracy Drop: {importance[\"accuracy_drop\"]:.4f}')\n",
    "    print(f'  Precision Drop: {importance[\"precision_drop\"]:.4f}')\n",
    "    print(f'  Recall Drop: {importance[\"recall_drop\"]:.4f}')\n",
    "    print(f'  F1 Drop: {importance[\"f1_drop\"]:.4f}')\n",
    "    print(f'  Kappa Drop: {importance[\"kappa_drop\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days rising: 108\n",
      "Number of days falling: 67\n",
      "Rise % is: 61.71%\n",
      "Fall % is: 38.29%\n"
     ]
    }
   ],
   "source": [
    "fall = (ticker.Target == 0).sum()\n",
    "rise = (ticker.Target == 1).sum()\n",
    "\n",
    "print(f'Number of days rising: {rise}')\n",
    "print(f'Number of days falling: {fall}')\n",
    "\n",
    "print(f'Rise % is: {(rise / (rise + fall)) * 100:.2f}%')\n",
    "print(f'Fall % is: {(fall / (rise + fall)) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nyse_daily</th>\n",
       "      <td>0.514634</td>\n",
       "      <td>0.567364</td>\n",
       "      <td>0.533256</td>\n",
       "      <td>0.542568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Average Accuracy  Average Precision  Average Recall  Average F1\n",
       "Dataset                                                                    \n",
       "nyse_daily          0.514634           0.567364        0.533256    0.542568"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Dataset', 'Average Accuracy', 'Average Precision', \n",
    "                                'Average Recall', 'Average F1'])\n",
    "\n",
    "datatset_name = file_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "metrics = {'Dataset': datatset_name ,'Average Accuracy': avg_accuracy, \n",
    "           'Average Precision': avg_precision, 'Average Recall': avg_recall, \n",
    "           'Average F1': avg_f1}\n",
    "\n",
    "export_results = results._append(metrics, ignore_index=True)\n",
    "\n",
    "export_results.set_index('Dataset', inplace=True)\n",
    "\n",
    "export_results.to_csv(r\"C:\\Users\\samto\\Desktop\\IRP DATA-20240724T195545Z-001\\IRP DATA\\\\\" + datatset_name + '_results.csv')\n",
    "\n",
    "export_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
