{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.7018, Validation Error: 0.7027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 266\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m    265\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[1;32m--> 266\u001b[0m train_errors, val_errors \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Evaluate on Training Data\u001b[39;00m\n\u001b[0;32m    269\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, train_data)\n",
      "Cell \u001b[1;32mIn[3], line 126\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, train_data, val_data, loss_func, opt, epochs)\u001b[0m\n\u001b[0;32m    122\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    123\u001b[0m model\u001b[38;5;241m.\u001b[39mhidden_cell \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mhidden_layer),\n\u001b[0;32m    124\u001b[0m                      torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mhidden_layer))\n\u001b[1;32m--> 126\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(y, labels)\n\u001b[0;32m    128\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m, in \u001b[0;36mLSTM_Model.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_cell1 \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer), \n\u001b[0;32m     66\u001b[0m                     torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer))\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_cell2 \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer), \n\u001b[0;32m     68\u001b[0m                     torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer))\n\u001b[1;32m---> 70\u001b[0m out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_cell1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_cell1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(out)\n\u001b[0;32m     73\u001b[0m out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_cell2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_cell2)\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\samto\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    773\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas_ta as ta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize the Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.wm_attributes('-topmost', 1)\n",
    "root.withdraw()\n",
    "\n",
    "# Normalize data and assign movement direction values\n",
    "def prep(dataset):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    dataset['RSI (14D)'] = ta.rsi(dataset['Close'], length=14)\n",
    "    dataset['20 Day CCI'] = ta.cci(high=dataset['High'], low=dataset['Low'], \n",
    "                                   close=dataset['Close'], length=20)\n",
    "    dataset['Williams %R'] = ta.willr(high=dataset['High'], low=dataset['Low'], \n",
    "                                      close=dataset['Close'], length=14)\n",
    "    dataset['EMA (5D)'] = dataset['Close'].ewm(span=5, adjust=False).mean()\n",
    "\n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Volume', 'RSI (14D)', \n",
    "                '20 Day CCI', 'Williams %R', 'Mortgage_rate', 'Unemp_rate',\n",
    "                'disposable_income', 'Personal_consumption_expenditure', \n",
    "                'personal_savings']\n",
    "    \n",
    "    dataset[features] = dataset[features].astype(float)\n",
    "    dataset[features] = scaler.fit_transform(dataset[features])\n",
    "\n",
    "    dataset['MA10'] = dataset['Close'].rolling(window=10).mean()\n",
    "    dataset['MA50'] = dataset['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    dataset['Target'] = np.where(dataset['Close'].shift(-1) > dataset['Close'], 1, 0)\n",
    "    prepared_data = dataset.dropna()\n",
    "    return prepared_data, scaler\n",
    "\n",
    "# Define LSTM Model class\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.lstm1 = nn.LSTM(input_layer, hidden_layer, batch_first=True, bidirectional=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lstm2 = nn.LSTM(hidden_layer, hidden_layer, batch_first=True, bidirectional=False)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear_layer = nn.Linear(hidden_layer, output_layer)\n",
    "        self.hidden_cell1 = (torch.zeros(1,1,self.hidden_layer),\n",
    "                            torch.zeros(1,1,self.hidden_layer))\n",
    "        self.hidden_cell2 = (torch.zeros(1,1,self.hidden_layer),\n",
    "                            torch.zeros(1,1,self.hidden_layer))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        self.hidden_cell1 = (torch.zeros(1, input_tensor.size(0), self.hidden_layer), \n",
    "                            torch.zeros(1, input_tensor.size(0), self.hidden_layer))\n",
    "        self.hidden_cell2 = (torch.zeros(1, input_tensor.size(0), self.hidden_layer), \n",
    "                            torch.zeros(1, input_tensor.size(0), self.hidden_layer))\n",
    "        \n",
    "        out, self.hidden_cell1 = self.lstm1(input_tensor, self.hidden_cell1)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out, self.hidden_cell2 = self.lstm2(out, self.hidden_cell2)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        lstm_out_last = out[:, -1, :]\n",
    "\n",
    "        output = self.linear_layer(lstm_out_last)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Create sequences for input data and corresponding labels\n",
    "def create_sequence(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(input_data) - sequence_length):\n",
    "        sequence = input_data[i : i + sequence_length, :-1]\n",
    "        label = input_data[i + sequence_length, -1]\n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return sequences, labels\n",
    "\n",
    "# Convert sequences and labels to PyTorch tensors\n",
    "def convert_to_tensor(sequences, labels):\n",
    "    # Convert sequences to tensors if they are not already\n",
    "    if not isinstance(sequences[0], torch.Tensor):\n",
    "        sequences = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "    \n",
    "    # Convert labels to tensor and reshape\n",
    "    if not isinstance(labels, torch.Tensor):\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n",
    "    else:\n",
    "        labels = labels.view(-1, 1)  # Ensure the correct shape if already a tensor\n",
    "\n",
    "    # Pad sequences\n",
    "    sequences = pad_sequence(sequences, batch_first=True)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "# Train the model with data provided\n",
    "def trainer(model, train_data, val_data, loss_func, opt, epochs):\n",
    "    train_errors, val_errors = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for sequence, labels in train_data:\n",
    "            opt.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer),\n",
    "                                 torch.zeros(1, 1, model.hidden_layer))\n",
    "            \n",
    "            y = model(sequence)\n",
    "            loss = loss_func(y, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        train_errors.append(epoch_loss / len(train_data))\n",
    "\n",
    "        # Validate the model\n",
    "        val_metrics = evaluate_model(model, val_data)\n",
    "        val_accuracy = val_metrics['accuracy']  # Extract accuracy\n",
    "        val_errors.append(1 - val_accuracy)  # Convert accuracy to error\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 25 == 1:\n",
    "            print(f'Epoch {epoch}, Training Loss: {epoch_loss / len(train_data):.4f}, Validation Error: {1 - val_accuracy:.4f}')\n",
    "    \n",
    "    return train_errors, val_errors\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequence, label in data_loader:\n",
    "            sequence = sequence.float()\n",
    "            output = model(sequence)\n",
    "            pred = torch.round(torch.sigmoid(output)).cpu().numpy()\n",
    "            predictions.extend(pred.flatten())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays for sklearn metric functions\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    \n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, zero_division=0)\n",
    "    recall = recall_score(labels, predictions, zero_division=0)\n",
    "    f1 = f1_score(labels, predictions, zero_division=0)\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "def time_series_cv(model_class, sequences, labels, n_splits, epochs):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    i = 1\n",
    "\n",
    "    for train_index, test_index in tscv.split(sequences):\n",
    "        train_sequences, test_sequences = [sequences[i] for i in train_index], [sequences[i] for i in test_index]\n",
    "        train_labels, test_labels = [labels[i] for i in train_index], [labels[i] for i in test_index]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        train_sequences, train_labels = convert_to_tensor(train_sequences, train_labels)\n",
    "        test_sequences, test_labels = convert_to_tensor(test_sequences, test_labels)\n",
    "\n",
    "        # Create dataLoaders\n",
    "        train_data = torch.utils.data.DataLoader(list(zip(train_sequences, train_labels)), shuffle=True, batch_size=100)\n",
    "        test_data = torch.utils.data.DataLoader(list(zip(test_sequences, test_labels)), shuffle=False, batch_size=100)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = model_class(input_layer=15, hidden_layer=50, output_layer=1)\n",
    "        opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Train and evaluate\n",
    "        train_errors, val_errors = trainer(model, train_data, test_data, loss_func, opt, epochs)\n",
    "        test_metrics = evaluate_model(model, test_data)\n",
    "        \n",
    "        accuracies.append(test_metrics['accuracy'])\n",
    "        precisions.append(test_metrics['precision'])\n",
    "        recalls.append(test_metrics['recall'])\n",
    "        f1_scores.append(test_metrics['f1_score'])\n",
    "\n",
    "        print(f'\\nAvg cross-validation metrics for fold {i}: Accuracy: {accuracies[-1] * 100:.2f}%, '\n",
    "              f'Precision: {precisions[-1]:.4f}, Recall: {recalls[-1]:.4f}, F1 Score: {f1_scores[-1]:.4f}\\n')\n",
    "\n",
    "        print(f'\\nAvg cross-validation accuracy for fold {i}: {accuracies[-1] * 100}% \\n')\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    return accuracies, precisions, recalls, f1_scores\n",
    "\n",
    "# Load and prepare data\n",
    "file_path = filedialog.askopenfilename(parent=root, title=\"Select A File\")\n",
    "ticker = pd.read_csv(file_path)\n",
    "ticker, scaler = prep(ticker)\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 50\n",
    "sequences, labels = create_sequence(ticker[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI (14D)', \n",
    "                '20 Day CCI', 'Williams %R', 'Mortgage_rate', 'Unemp_rate',\n",
    "                'disposable_income', 'Personal_consumption_expenditure', \n",
    "                'personal_savings', 'MA10', 'MA50', 'Target']].values, sequence_length)\n",
    "\n",
    "# Convert sequences and labels to tensors\n",
    "sequences, labels = convert_to_tensor(sequences, labels)\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_size = int(len(sequences) * 0.6)\n",
    "val_size = int(len(sequences) * 0.2)\n",
    "train_sequences = sequences[:train_size]\n",
    "val_sequences = sequences[train_size:train_size + val_size]\n",
    "test_sequences = sequences[train_size + val_size:]\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(list(zip(train_sequences, labels[:train_size])), shuffle=True, batch_size=100)\n",
    "val_data = torch.utils.data.DataLoader(list(zip(val_sequences, labels[train_size:train_size + val_size])), shuffle=False, batch_size=100)\n",
    "test_data = torch.utils.data.DataLoader(list(zip(test_sequences, labels[train_size + val_size:])), shuffle=False, batch_size=100)\n",
    "\n",
    "# Initialize model\n",
    "model = LSTM_Model(input_layer=15, hidden_layer=50, output_layer=1)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Train\n",
    "epochs = 150\n",
    "train_errors, val_errors = trainer(model, train_data, val_data, loss_func, opt, epochs)\n",
    "\n",
    "# Evaluate on Training Data\n",
    "train_metrics = evaluate_model(model, train_data)\n",
    "print(f'Training Metrics - Accuracy: {train_metrics[\"accuracy\"]:.4f}, '\n",
    "      f'Precision: {train_metrics[\"precision\"]:.4f}, Recall: {train_metrics[\"recall\"]:.4f}, '\n",
    "      f'F1 Score: {train_metrics[\"f1_score\"]:.4f}')\n",
    "\n",
    "# Plot confusion matrix for training data\n",
    "print(\"Confusion Matrix for Training Data\")\n",
    "plot_confusion_matrix(train_metrics['confusion_matrix'], labels=['Class 0', 'Class 1'])\n",
    "\n",
    "# Evaluate on Test Data\n",
    "test_metrics = evaluate_model(model, test_data)\n",
    "print(f'Test Metrics - Accuracy: {test_metrics[\"accuracy\"]:.4f}, '\n",
    "      f'Precision: {test_metrics[\"precision\"]:.4f}, Recall: {test_metrics[\"recall\"]:.4f}, '\n",
    "      f'F1 Score: {test_metrics[\"f1_score\"]:.4f}')\n",
    "\n",
    "# Plot confusion matrix for test data\n",
    "print(\"Confusion Matrix for Test Data\")\n",
    "plot_confusion_matrix(test_metrics['confusion_matrix'], labels=['Class 0', 'Class 1'])\n",
    "\n",
    "# Perform Time Series Cross-Validation\n",
    "n_splits = 10\n",
    "accuracies, precisions, recalls, f1_scores = time_series_cv(LSTM_Model, sequences, labels, n_splits, epochs)\n",
    "print(f'Cross-Validation Accuracies: {accuracies}')\n",
    "print(f'Cross-Validation Precisions: {precisions}')\n",
    "print(f'Cross-Validation Recalls: {recalls}')\n",
    "print(f'Cross-Validation F1 Scores: {f1_scores}')\n",
    "print(f'Average Cross-Validation Accuracy: {np.mean(accuracies) * 100:.2f}%')\n",
    "print(f'Average Cross-Validation Precision: {np.mean(precisions):.4f}')\n",
    "print(f'Average Cross-Validation Recall: {np.mean(recalls):.4f}')\n",
    "print(f'Average Cross-Validation F1 Score: {np.mean(f1_scores):.4f}')\n",
    "\n",
    "# Plotting cross-validation accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_splits + 1), accuracies, marker='o')\n",
    "plt.xlabel('Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-Validation Accuracy Across Folds')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
